{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.functional as F\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence): \n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    sentence = stemmer.stem(sentence) #remove endings\n",
    "    sentence = sentence.lower() #for writing from small litter\n",
    "    tokenizer = RegexpTokenizer(r'\\w+') #  #for removing puntuations\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    filtered_words = [wordnet_lemmatizer.lemmatize(word) for word in tokens]\n",
    "    filtered_words = [w for w in tokens if not w in stopwords.words('english')] #remove stop words\n",
    "    filtered_words = [c for c in filtered_words if not c.isdigit()] #remove numbers\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  category                                               text\n",
      "0     tech  tv future in the hands of viewers with home th...\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "url=\"bbc.csv\"\n",
    "dataset = pd.read_csv(url)\n",
    "dataset = dataset[dataset.category == 'tech']\n",
    "dataset.index = np.arange(len(dataset))\n",
    "dataset = dataset[:1]\n",
    "print(dataset.head())\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    tv future in the hands of viewers with home th...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "corpus = dataset['text']\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = []\n",
    "stemmer = PorterStemmer()\n",
    "for i in range(len(corpus)):\n",
    "    sentence = preprocess(corpus[i])\n",
    "    tokenized_corpus.append(word_tokenize((sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = []\n",
    "for sentence in tokenized_corpus:\n",
    "    for token in sentence:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "vocabulary_size = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 1\n",
    "idx_pairs = []\n",
    "# for each sentence\n",
    "for sentence in tokenized_corpus:\n",
    "    indices = [word2idx[word] for word in sentence]\n",
    "    # for each word, threated as center word\n",
    "    for center_word_pos in range(len(indices)):\n",
    "        # for each window position\n",
    "        for w in range(-window_size, window_size + 1):\n",
    "            context_word_pos = center_word_pos + w\n",
    "            # make soure not jump out sentence\n",
    "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                continue\n",
    "            context_word_idx = indices[context_word_pos]\n",
    "            idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Создаем пустую матрицу \n",
    "array_occurance = [[0 for j in range(len(vocabulary))] for i in range(len(vocabulary))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(vocabulary)):\n",
    "    for j in range(len(vocabulary)):\n",
    "        for m in range (len(idx_pairs)):\n",
    "            if (i != j):\n",
    "                if (idx_pairs[i][0] == idx_pairs[m][0]) and (idx_pairs[j][1] == idx_pairs[m][1]):\n",
    "                    array_occurance[i][j] = array_occurance[i][j] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = []\n",
    "target = []\n",
    "weight = []\n",
    "vocabulary_new = []\n",
    "for i in range(len(array_occurance)):\n",
    "    for j in range(len(array_occurance)):\n",
    "        if (array_occurance[i][j] > 2):\n",
    "            source.append(vocabulary[i])\n",
    "            target.append(vocabulary[j])\n",
    "            weight.append(array_occurance[i][j])\n",
    "            if (vocabulary[i]) not in vocabulary_new:\n",
    "                vocabulary_new.append(vocabulary[i])\n",
    "            if (vocabulary[j]) not in vocabulary_new:\n",
    "                vocabulary_new.append(vocabulary[j])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ({\"source\", \"target\",\"weight\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['source'] = source\n",
    "df['target'] = target\n",
    "df['weight'] = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'edgelist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(columns = ({\"id\", \"role\",\"salary\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['id'] = vocabulary_new\n",
    "df2['role'] = 'word'\n",
    "df2['salary'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(r'nodelist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
